{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\userspace\\daya\\python\\samu_proj\\pcos_preprocessed.csv')\n",
    "X = df.drop(columns=['PCOS (Y/N)'])  # Features\n",
    "y = df['PCOS (Y/N)'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),  # Increase max_iter\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'Support Vector Machine': SVC(max_iter=5000),\n",
    "    'LinearSVC': LinearSVC(max_iter=5000),  # Increase max_iter for LinearSVC\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Gaussian Naive Bayes': GaussianNB()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91        77\n",
      "           1       0.81      0.69      0.75        32\n",
      "\n",
      "    accuracy                           0.86       109\n",
      "   macro avg       0.85      0.81      0.83       109\n",
      "weighted avg       0.86      0.86      0.86       109\n",
      "\n",
      "============================================================\n",
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89        77\n",
      "           1       0.77      0.62      0.69        32\n",
      "\n",
      "    accuracy                           0.83       109\n",
      "   macro avg       0.81      0.77      0.79       109\n",
      "weighted avg       0.83      0.83      0.83       109\n",
      "\n",
      "============================================================\n",
      "Classification Report for Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.94      0.87        77\n",
      "           1       0.75      0.47      0.58        32\n",
      "\n",
      "    accuracy                           0.80       109\n",
      "   macro avg       0.78      0.70      0.72       109\n",
      "weighted avg       0.79      0.80      0.78       109\n",
      "\n",
      "============================================================\n",
      "Classification Report for Support Vector Machine:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      1.00      0.83        77\n",
      "           1       1.00      0.00      0.00        32\n",
      "\n",
      "    accuracy                           0.71       109\n",
      "   macro avg       0.85      0.50      0.41       109\n",
      "weighted avg       0.79      0.71      0.58       109\n",
      "\n",
      "============================================================\n",
      "Classification Report for LinearSVC:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.88        77\n",
      "           1       0.81      0.53      0.64        32\n",
      "\n",
      "    accuracy                           0.83       109\n",
      "   macro avg       0.82      0.74      0.76       109\n",
      "weighted avg       0.82      0.83      0.81       109\n",
      "\n",
      "============================================================\n",
      "Classification Report for K-Nearest Neighbors:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.86      0.80        77\n",
      "           1       0.50      0.34      0.41        32\n",
      "\n",
      "    accuracy                           0.71       109\n",
      "   macro avg       0.63      0.60      0.61       109\n",
      "weighted avg       0.68      0.71      0.69       109\n",
      "\n",
      "============================================================\n",
      "Classification Report for Decision Tree:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83        77\n",
      "           1       0.60      0.47      0.53        32\n",
      "\n",
      "    accuracy                           0.75       109\n",
      "   macro avg       0.70      0.67      0.68       109\n",
      "weighted avg       0.74      0.75      0.74       109\n",
      "\n",
      "============================================================\n",
      "Classification Report for Gaussian Naive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91        77\n",
      "           1       0.78      0.78      0.78        32\n",
      "\n",
      "    accuracy                           0.87       109\n",
      "   macro avg       0.85      0.85      0.85       109\n",
      "weighted avg       0.87      0.87      0.87       109\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\application\\anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    if name == 'Logistic Regression':\n",
    "        # Fit logistic regression model on scaled data\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"Classification Report for {name}:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))  # Set zero_division parameter to 1\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
